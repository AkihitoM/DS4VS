{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249b9306",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 14: an overview of Neural Networks and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef247fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Brief Recap:\n",
    "\n",
    "* Hello, how are you?\n",
    "* Today: An overview of Neural Nets and Deep Learning\n",
    "* Next Class: I will listen to you!: Presentation time\n",
    "* After presentations I will spend the second half talking about the homeworks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a48908",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Artificial Neural Networks & Deep Learning\n",
    "\n",
    "**Artificial Neural Networks** - a collection of interconnected nodes called artificial neurons that follow simple rules to mimic the behavior of biological neurons  \n",
    "\n",
    "**Deep Neural Network** - an artificial neural network with multiple layers. a network with only one hidden layer is often called 'shallow'  \n",
    "\n",
    "**Deep Learning** - a type of machine learning with artificial neural networks arranged in multiple (deep) 'hidden' layers\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/OH3gI.png\" width=\"60%\" style=\"margin-left:auto; margin-right:auto\">  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c43d594",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## a stroll through the history of Neural Nets\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"http://beamlab.org/images/deep_learning_101/nn_timeline.jpg\" width=\"100%\" style=\"margin-left:auto; margin-right:auto\">  \n",
    "\n",
    "image cred: [beam lab](http://beamlab.org/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24255539",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Logical Calculus of Neural Activity\n",
    "\n",
    "Developed by McCulloch & Pitts (1943)\n",
    "\n",
    "***\n",
    "\n",
    "* **\"all-or-none\"** law $\\therefore$ any neuron may be represented as a proposition\n",
    "* **proposition** - a declarative statement (either `True` or `False`)\n",
    "* **propositional calculus** constructing arguments from propositions by using logical connectives\n",
    "\n",
    "McCulloch & Pitts viewed neurons as propositional units and their physical connection with axons analogous to logical connections that can be used to buils arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d834fa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Theory: Nets Without Circles\n",
    "\n",
    "1. neural activity is \"all-or-none\"\n",
    "2. a certain number of synapses within a certain time period must excite a neuron for it to respond\n",
    "3. the only significant delay is synaptic delay\n",
    "4. activity of inhibitory synapses prevents a neuron from being active\n",
    "5. the structure of the net does not change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab8de7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### McCulloch & Pitts Neuron\n",
    "\n",
    "<img src=\"https://forum.huawei.com/enterprise/en/data/attachment/forum/202005/24/200534rcs8ro8hh4dwv26d.png?ann.png\" width=\"100%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc86508",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### McCulloch & Pitts Components\n",
    "\n",
    "* directed weighted paths (positive or negative)\n",
    "* Transfer function (sum the inputs)\n",
    "* Activation function (Binary: fire/don't fire) applied over the net input\n",
    "    * achieve a non-linearity\n",
    "    * $f(x) = \\left\\{ \\begin{array}{rcl}\n",
    "1 & \\mbox{If}\n",
    "& x \\geq \\Theta \\\\ 0 & \\mbox{If} & x \\lt \\Theta\n",
    "\\end{array}\\right.$\n",
    "* Threshold ($\\Theta$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4050d5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Building Logic Gates with McCulloch & Pitts Neuron\n",
    "\n",
    "combinations of these simple logic gates can be combined to perform some complex expressions\n",
    "\n",
    "<img src=\"https://slideplayer.org/slide/890210/3/images/57/McCulloch-Pitts-Neuronen+zum+Aufbau+logischer+Funktionen.jpg\" width=\"60%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d19a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Perceptron\n",
    "\n",
    "Developed by Frank Rensenblatt (1957)  \n",
    "\n",
    "***\n",
    "\n",
    "**Threshold Logic Unit (TLU)** a simple ANN architecture that build from McCulloch & Pitts  \n",
    "\n",
    "* inputs and outputs are numbers (not binary on/off)\n",
    "* each input has an associated weight\n",
    "* computes a weighted sum of the inputs (transfer fxn)\n",
    "    - $\\sum_1^n (w_nx_n)$\n",
    "    - $\\mathbf{x}^T\\mathbf{w}$\n",
    "* applies a step function (activation fxn) \n",
    "    - heaviside $(z) = \\left\\{ \\begin{array}{rcl}\n",
    "0 & \\mbox{If}\n",
    "& z \\lt 0 \\\\ 1 & \\mbox{If} & z \\geq 0\n",
    "\\end{array}\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea1e9f1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Perceptron Architecture\n",
    "\n",
    "* a layer of 'passthrough' input neurons (with a bias feature)\n",
    "* a single, fully connected Perceptron layer\n",
    "* output layer\n",
    "\n",
    "<img src=\"https://data-science-blog.com/wp-content/uploads/2020/07/perceptron_1-1030x501.png\" width=\"60%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2e843b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A clearer Perceptron\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Carlo-Sansone/publication/280696016/figure/fig6/AS:668447369343000@1536381680862/An-example-three-layer-Perceptron-This-architecture-may-be-used-to-make-a-binary.ppm\" width=\"70%\" style=\"margin-left:auto; margin-right:auto\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674a2dff",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training Perceptrons\n",
    "\n",
    "Hebbian Learning: Cells that fire together, wire together\n",
    "\n",
    "***\n",
    "\n",
    "Perceptron uses a learning rule similar to Hebbian learning.  \n",
    "Connections are reinforced to help reduce error:\n",
    "\n",
    "    for each training instance:\n",
    "    make a prediction. ...activation_fxn(inputs*Weights+bias)\n",
    "    for every output neuron that made a wrong prediction:\n",
    "    strengthen the weights that would have produced the correct prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6624aa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Perceptron Performance\n",
    "\n",
    "The sigmoid step function is equivalent to the logistic function.  \n",
    "In turn, Perceptron has a linear decision boundary.  \n",
    "Perceptron cannot learn complex patterns.  \n",
    "However, it can find an optimal solution for linear separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf1b263",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's look at a quick `sklearn` Perceptron example ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa47c5",
   "metadata": {},
   "source": [
    "## Adaline: Adaptive Linear Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5443fa2d",
   "metadata": {},
   "source": [
    "<img src=\"https://content.techgig.com/photo/80071467/pros-and-cons-of-python-programming-language-that-every-learner-must-know.jpg?132269\" width=\"100%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca3c71d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[play with a Neural Network](https://playground.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da39ce83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ðŸ¥º The! Last! Lecture! ðŸ˜¢\n",
    "<img src=\"https://content.techgig.com/photo/80071467/pros-and-cons-of-python-programming-language-that-every-learner-must-know.jpg?132269\" width=\"100%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
