{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bf8fc74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 12: Unsupervised Learning \n",
    "\n",
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae905762",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## a Brief Recap:\n",
    "\n",
    "* Hello, how are you?\n",
    "* Today: Dimensionality Reductions, Unsupervised Learning Methods for Numeric Targets\n",
    "* Next week: Culstering, Unsupervised Learning for Categorical Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa33315b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "What if we have some data in the form of a feature space, but no information on the target variable?  \n",
    "\n",
    "Unsupervised Learning Methods do not get feedback on a target, rather, unsupervised learning seeks to discover patterns in the features provided.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae8c33a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Families of Unsupervised Methods:  \n",
    "\n",
    "* Dimensionality Reduction \n",
    "    - PCA - principle component analysis\n",
    "    - LDA - linear discirminant analysis\n",
    "    - SVD - singular value decomposition\n",
    "* Clustering\n",
    "    - k-Means\n",
    "    - Hierarchical lustering\n",
    "    - Gaussian Mixture Densities\n",
    "* Non-parametric\n",
    "    - kernel estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11794980",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "Reducing the dimensionality (degrees of freedom) of a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ad449",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Curse of Dimensionality\n",
    "\n",
    "<img src=\"https://eranraviv.com/wp-content/uploads/2016/01/COD.png\" width=\"70%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1b3b8e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A Cuter Curse\n",
    "\n",
    "<img src=\"https://www.visiondummy.com/wp-content/uploads/2014/04/curseofdimensionality.png\" width=\"70%\" style=\"margin-left:auto; margin-right:auto\">\n",
    "\n",
    "[computer vision for dummies link](https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3876c22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why reduce the dimension space; isn't more information better?  \n",
    "\n",
    "* high dimensional datasets are very sparse!\n",
    "* remove uninformative features to increase the power of inferences made during testing\n",
    "* simple models are more robust, because the performance varies less\n",
    "* simple models are easier to interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae910f36",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### How to reduce the dimension space\n",
    "\n",
    "* **feature selection** - if all features are independent, use a criteria to subset features relevant to the model\n",
    "    - ex: forward or backward features selection\n",
    "* **feature extraction** - finding a new set of dimensions that are derived from the dataset features\n",
    "    - **Projection** - find a subspace of the features that explains as much of the variance as possible\n",
    "    - **Manifold Learning** - a **manifold** is part of an $n$-dimensional space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4898d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA\n",
    "\n",
    "**PCA** - Principle Component Analysis. We are interested in finding a mapping from the original dataset feature space with $d$ degress of freedom to a new space with $k$ degrees of freedom where $k\\lt d$; we would like to minimize the loss of information in the process\n",
    "\n",
    "* map to a smaller feature space\n",
    "* try to preserve information\n",
    "* obtain the uncorrelated components of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ab7796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a730c381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d81660a4",
   "metadata": {},
   "source": [
    "### PCA Algorithm\n",
    "\n",
    "1. Center the Data\n",
    "2. Normalize the Data\n",
    "3. Calculate the Eigendecomposition\n",
    "4. Project the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543bfdce",
   "metadata": {},
   "source": [
    "## Next week we'll look at Clustering: Unsupervised Learning for Categorical Targets\n",
    "<img src=\"https://content.techgig.com/photo/80071467/pros-and-cons-of-python-programming-language-that-every-learner-must-know.jpg?132269\" width=\"100%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
