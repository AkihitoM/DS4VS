{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be476bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b492cad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 10: Regularized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747420b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## On Noise\n",
    "\n",
    "$$Y \\approx \\beta_0 + \\beta_1X + \\beta_2X + \\dots + \\beta_NX$$\n",
    "\n",
    "Linear Regression finds the input-output relationahip as a weighted sum of the predictors.  \n",
    "However, the data is not perfect.   \n",
    "There is necessarily error/noise present  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af1a6b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**A Multiple Linear Regression Phenomenon**  \n",
    "For a training given dataset, as more features are added to a model the $R^2$ increases even if the added parameter in uninformative.  \n",
    "At a certain point, adding new parameters fits the model to the noise inherent in the data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ac88bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Bias Variance Trade-off\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1838/1*1BGl9kfU6nwO2QQ0-fWHcg.png\" width=\"60%\" style=\"margin-left:auto; margin-right:auto\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eade9a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generalization Error\n",
    "\n",
    "**Generalization Error** - a measure of how accurately a model can predict previously unseen data  \n",
    "\n",
    "Comparing measures generalization is informative of the optimal model complexity\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/0NbOY.png\" width=\"80%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c3c660",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img src=\"https://miro.medium.com/max/875/0*XCe3mlLeGiUW3xfh\" width=\"60%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e62e4e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization: bringing to uniformity\n",
    "\n",
    "**Regularized Linear Models**  \n",
    "\n",
    "* Regularize a model to reduce overfitting: constrain it somehow\n",
    "* For Linear Regression this means: constrain the weights (parameters) of the model. \n",
    "* This is usually implemented by adding a regularization term to the cost function\n",
    "\n",
    "Today we will survey 3 regularization methods for linear models  \n",
    "\n",
    "1. Ridge Regression\n",
    "2. Lasso Regression\n",
    "3. Elastic Net Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1905e430",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Revisit the NYC Italian Restaurant Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "418ef5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as np\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "path = 'https://raw.githubusercontent.com/SmilodonCub/DS4VS/master/datasets/nyc.csv'\n",
    "df = pd.read_csv( path, encoding= 'unicode_escape' )\n",
    "\n",
    "X = df.drop(['Price', 'Case', 'Restaurant'], axis=1)\n",
    "y = df['Price']\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c9ef9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use `sklearn` to build a 'kitchen sink' MLR\n",
    "\n",
    "we will use this both to see how MLR is done with `sklearn` and to compare performance with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e467f1d4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-25.883634392769906 [1.39643611 1.87937135 0.29545527 1.67469429]\n"
     ]
    }
   ],
   "source": [
    "# instantiate a Linear Regression Model\n",
    "lin_mod = LinearRegression()\n",
    "# fit the model to the training data\n",
    "lin_mod.fit( X_train, y_train )\n",
    "# print the model intercept & coefficients\n",
    "print( lin_mod.intercept_, lin_mod.coef_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94866ccc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Evaluate the Model Performance on unseen data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c7e136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "010c9b91",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1) Ridge Regression\n",
    "\n",
    "**Ridge Regression**  \n",
    "\n",
    "- add a term to the cost function that froces the model to minimize the model weights. \n",
    "- **Cost Function** $J(\\theta) = \\mbox{MSE}(\\theta) + \\alpha \\frac{1}{2}\\sum_{i=1}^n \\theta_i^2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c6527dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-25.86340306756169"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\", random_state=42)\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "ridge_reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f03e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0eb2f705",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next week: Supervised Learning techniques for Categorical Target Variables\n",
    "<img src=\"https://content.techgig.com/photo/80071467/pros-and-cons-of-python-programming-language-that-every-learner-must-know.jpg?132269\" width=\"100%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
