{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed7404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e90e1cfa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 10: Regularized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e165a67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## On Noise\n",
    "\n",
    "$$Y \\approx \\beta_0 + \\beta_1X + \\beta_2X + \\dots + \\beta_NX$$\n",
    "\n",
    "Linear Regression finds the input-output relationahip as a weighted sum of the predictors.  \n",
    "However, the data is not perfect.   \n",
    "There is necessarily error/noise present  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2f301e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**A Multiple Linear Regression Phenomenon**  \n",
    "For a training given dataset, as more features are added to a model the $R^2$ increases even if the added parameter in uninformative.  \n",
    "At a certain point, adding new parameters fits the model to the noise inherent in the data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58191306",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Bias Variance Trade-off\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1838/1*1BGl9kfU6nwO2QQ0-fWHcg.png\" width=\"60%\" style=\"margin-left:auto; margin-right:auto\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fa1f6a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generalization Error\n",
    "\n",
    "**Generalization Error** - a measure of how accurately a model can predict previously unseen data  \n",
    "\n",
    "Comparing measures generalization is informative of the optimal model complexity\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/0NbOY.png\" width=\"80%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6c66d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img src=\"https://miro.medium.com/max/875/0*XCe3mlLeGiUW3xfh\" width=\"60%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d18e549",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization: bringing to uniformity\n",
    "\n",
    "**Regularized Linear Models**  \n",
    "\n",
    "* Regularize a model to reduce overfitting: constrain it somehow\n",
    "* For Linear Regression this means: constrain the weights (parameters) of the model. \n",
    "* This is usually implemented by adding a regularization term to the cost function\n",
    "\n",
    "Today we will survey regularization methods for linear models  \n",
    "\n",
    "1. Ridge Regression\n",
    "2. Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8303baa2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Revisit the NYC Italian Restaurant Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as np\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "path = 'https://raw.githubusercontent.com/SmilodonCub/DS4VS/master/datasets/nyc.csv'\n",
    "df = pd.read_csv( path, encoding= 'unicode_escape' )\n",
    "\n",
    "X = df.drop(['Price', 'Case', 'Restaurant'], axis=1)\n",
    "y = df['Price']\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_scaled = sc.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(data = X_scaled, columns = X.columns)\n",
    "\n",
    "X_train, X_test, X_scaled_train, X_scaled_test, y_train, y_test = train_test_split( X, X_scaled, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4006bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use `sklearn` to build a 'kitchen sink' MLR\n",
    "\n",
    "we will use this both to see how MLR is done with `sklearn` and to compare performance with Regularization\n",
    "\n",
    "`LinearRegression()` will implement OLS. OLS is ideal when the underlying relationship is Linear and we have n>>p. But if n is not much larger than p or p>n (unfeasible for OLS), there can be a lot of variability in the fit which can result in either overfitting and very poor predictive ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81d2e8b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate a Linear Regression Model\n",
    "lin_mod = LinearRegression()\n",
    "# fit the model to the training data\n",
    "lin_mod.fit( X_train, y_train )\n",
    "# print the model intercept & coefficients\n",
    "print( lin_mod.intercept_, lin_mod.coef_ )\n",
    "# print the training R2 score\n",
    "score=r2_score(y_train,lin_mod.predict(X_train))\n",
    "print( 'r2 Training score is ', score )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09753922",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluate the Model Performance on unseen data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12855a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# use the model to make predictions on the test dataset\n",
    "y_prediction = lin_mod.predict(X_test)\n",
    "# predicting the accuracy score\n",
    "score=r2_score(y_test,y_prediction)\n",
    "print( 'Test data')\n",
    "print('r2 score is ',score)\n",
    "print('mean_sqrd_error is==', mean_squared_error(y_test,y_prediction))\n",
    "print('root_mean_squared error of is==', np.sqrt(mean_squared_error(y_test,y_prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1089ddd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1) Ridge Regression\n",
    "\n",
    "**Ridge Regression**  \n",
    "\n",
    "- add a term to the cost function that froces the model to minimize the model weights. \n",
    "- **Cost Function** $J(\\theta) = \\mbox{MSE}(\\theta) + \\alpha \\frac{1}{2}\\sum_{i=1}^n \\theta_i^2$\n",
    "- half the square of the $l_2$norm\n",
    "- **$\\alpha$** - a hyperparameter that controls the minimization\n",
    "    * $\\alpha$ == 0 is basically MLR\n",
    "    * $\\alpha$ is large, and the weights are close to zero (regress to bias) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c554a796",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\", random_state=42)\n",
    "ridge_reg.fit(X_scaled_train, y_train)\n",
    "print( ridge_reg.intercept_, ridge_reg.coef_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3e9aee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# use the model to make predictions on the test dataset\n",
    "ridgey_prediction = ridge_reg.predict( X_scaled_test )\n",
    "# predicting the accuracy score\n",
    "score=r2_score(y_test,ridgey_prediction)\n",
    "print('r2 score is ',score)\n",
    "print('mean_sqrd_error is==', mean_squared_error(y_test,ridgey_prediction))\n",
    "print('root_mean_squared error of is==', np.sqrt(mean_squared_error(y_test,ridgey_prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee99fd5b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### HHhhmmmmmm\n",
    "\n",
    "that $R^2$ score is not impressive!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abea2c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hyperparameter $\\alpha$\n",
    "\n",
    "We could just randomly try $\\alpha$s until we get a good result, but that would be inefficient and very biased.  \n",
    "`scikit-learn` will come to the rescue with the `GridSearchCV`  \n",
    "\n",
    "[`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) will perform a cross-validated sweep of a parameter space to find the best value for $\\alpha$  \n",
    "How convenient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ef3983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = Ridge()\n",
    "# define the parameter space\n",
    "#parameters = {'alpha':[1, 5, 10, 15, 20, 25, 50, 75, 100, 250, 500, 1000]}\n",
    "parameters = {'alpha':list(np.linspace(10.,20., 101))}\n",
    "# define the grid search\n",
    "Gridge_reg= GridSearchCV(model, parameters, scoring='neg_mean_squared_error',cv=5)\n",
    "#fit the grid search\n",
    "Gridge_reg.fit(X_scaled_train,y_train)\n",
    "# best estimator\n",
    "print(Gridge_reg.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ead6c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Best Gridsearch Model\n",
    "\n",
    "Let's use our best $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model\n",
    "best_Gridge_mod = Gridge_reg.best_estimator_\n",
    "best_Gridge_mod.fit(X_scaled_train,y_train)\n",
    "print( best_Gridge_mod.intercept_, best_Gridge_mod.coef_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc832533",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_Gridge_prediction = best_Gridge_mod.predict( X_scaled_test )\n",
    "score=r2_score(y_test,best_Gridge_prediction)\n",
    "print('r2 score is ',score)\n",
    "print('mean_sqrd_error is==', mean_squared_error(y_test,best_Gridge_prediction))\n",
    "print('root_mean_squared error of is==', np.sqrt(mean_squared_error(y_test,best_Gridge_prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7900e57c",
   "metadata": {},
   "source": [
    "## 2) Lasso Regression\n",
    "\n",
    "**Least Absolute Shrinkage and Selection Operator Regression** - Similar to Ridge, but adds the $l_1$ norm to the cost function  \n",
    "\n",
    "- **Cost Function** $J(\\theta) = \\mbox{MSE}(\\theta) + \\alpha \\sum_{i=1}^n |\\theta_i|$\n",
    "- tends to eliminate weights of unimportant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15057cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_reg = Lasso()\n",
    "#parameters = {'alpha':[0.001, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1, 2, 3, 4, 5, 10, 15, 20, 25, 50, 75]}\n",
    "parameters = {'alpha':list(np.linspace(0.2,0.4, 101))}\n",
    "# define the grid search\n",
    "Glasso_reg= GridSearchCV(lasso_reg, parameters, scoring='neg_mean_squared_error',cv=5)\n",
    "#fit the grid search\n",
    "Glasso_reg.fit(X_scaled_train,y_train)\n",
    "# best estimator\n",
    "print(Glasso_reg.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae04893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model\n",
    "best_Lasso_mod = Glasso_reg.best_estimator_\n",
    "best_Lasso_mod.fit(X_scaled_train,y_train)\n",
    "print( best_Lasso_mod.intercept_, best_Lasso_mod.coef_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3953d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_Lasso_prediction = best_Lasso_mod.predict( X_scaled_test )\n",
    "score=r2_score(y_test,best_Lasso_prediction)\n",
    "print('r2 score is ',score)\n",
    "print('mean_sqrd_error is==', mean_squared_error(y_test,best_Lasso_prediction))\n",
    "print('root_mean_squared error of is==', np.sqrt(mean_squared_error(y_test,best_Lasso_prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9a3551",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next week: Supervised Learning techniques for Categorical Target Variables\n",
    "<img src=\"https://content.techgig.com/photo/80071467/pros-and-cons-of-python-programming-language-that-every-learner-must-know.jpg?132269\" width=\"100%\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
